{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this tutorial, we will look at some of the tokenizers available in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "\n",
      "Good muffins cost Rs.3.80.\n",
      "in New York.  Please buy me two of them.\n",
      "\n",
      "Thanks.\n",
      "\n",
      "word_tokenize output\n",
      "['Good', 'muffins', 'cost', 'Rs.3.80', '.', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenization using NLTK\n",
    "# word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = \"Good muffins cost Rs.3.80.\\nin New York.  Please buy me two of them.\\n\\nThanks.\"\n",
    "print(\"Sentence: \\n\\n\"+s) \n",
    "print(\"\\nword_tokenize output\")\n",
    "print(word_tokenize(s))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "\n",
      "Good muffins cost $3.88\n",
      "in New York.  Please buy me two of them.\n",
      "\n",
      "Thanks.\n",
      "wordpunct_tokenize output\n",
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''\n",
    "print(\"Sentence: \\n\\n\"+s) \n",
    "print(\"wordpunct_tokenize output\")\n",
    "print(wordpunct_tokenize(s))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "\n",
      "abc is the context-based approach for morph-analysis. The accuracy of the approach is 77.80%\n",
      "\n",
      "sent_tokenize output\n",
      "['abc is the context-based approach for morph-analysis.', 'The accuracy of the approach is 77.80%']\n",
      "\n",
      "word_tokenize output\n",
      "['abc', 'is', 'the', 'context-based', 'approach', 'for', 'morph-analysis', '.']\n",
      "['The', 'accuracy', 'of', 'the', 'approach', 'is', '77.80', '%']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#s1 = '''Good muffins cost $3.88\\n to co-author in New York.  Please buy me two of them.\\n\\nThanks.'''\n",
    "s2= \"abc is the context-based approach for morph-analysis. The accuracy of the approach is 77.80%\"\n",
    "print(\"Sentence: \\n\\n\"+s2) \n",
    "print(\"\\nsent_tokenize output\")\n",
    "print(sent_tokenize(s2))\n",
    "print(\"\\nword_tokenize output\")\n",
    "for t in sent_tokenize(s2):\n",
    "    print(word_tokenize(t))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tokenization using NLTK\n",
    "\n",
    "# LineTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import LineTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LineTokenizer can be used to split strings containing newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: \n",
      "I love kites\n",
      "I like cricket\n",
      "I like football\n",
      "\n",
      "LineTokenizer...\n",
      "['I love kites', 'I like cricket', 'I like football']\n",
      "\n",
      "word_tokenizer... \n",
      "['I', 'love', 'kites']\n",
      "['I', 'like', 'cricket']\n",
      "['I', 'like', 'football']\n"
     ]
    }
   ],
   "source": [
    "s = \"I love kites\\nI like cricket\\nI like football\\n\"\n",
    "\n",
    "print(\"Sentences: \") \n",
    "print(s)\n",
    "print(\"LineTokenizer...\")\n",
    "print(LineTokenizer().tokenize(s))\n",
    "print(\"\\nword_tokenizer... \")\n",
    "for sent in LineTokenizer().tokenize(s):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegexpTokenizer allows us to provide regular expressions as delimiters\n",
    "The material between the tokens is discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Petrol price has gone upto Rs.75.89. 01/02/2017 I,John and Mrs. Thomas are thinking of using electric scooters.\n",
      "\n",
      "RegexpTokenizer...\n",
      "['Rs.75.89.']\n",
      "\n",
      "\n",
      "['Petrol', 'Rs.75.89.', 'I,John', 'Mrs.', 'Thomas']\n"
     ]
    }
   ],
   "source": [
    "s = \"Petrol price has gone upto Rs.75.89. 01/02/2017 I,John and Mrs. Thomas are thinking of using electric scooters.\"\n",
    "tokenizer = RegexpTokenizer('Rs\\.[\\d\\.]+\\S+')\n",
    "print(\"Sentence: \"+s)\n",
    "print(\"\\nRegexpTokenizer...\")\n",
    "print(tokenizer.tokenize(s))\n",
    "print(\"\\n\")\n",
    "#Let us say we want to extract all words beginning with an uppercase character\n",
    "capword_tokenizer = RegexpTokenizer('[A-Z]\\w*\\S+')\n",
    "print(capword_tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SExprTokenizer : Tokenizes parenthesized expressions in a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import SExprTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ?((a(b c)d)ef(g(h(i))))\n",
      "\n",
      "SExprTokenizer...\n",
      "['?', '((a(b c)d)ef(g(h(i))))']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = '?((a(b c)d)ef(g(h(i))))'\n",
    "print(\"Sentence: \"+s)\n",
    "print(\"\\nSExprTokenizer...\")\n",
    "print(SExprTokenizer().tokenize(s))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreebankWordTokenizer is standard tokenizer tool used and does a decent job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Petrol price has gone upto Rs.75.89 I,John and Mrs. Thomas are thinking of using electric scooters.\n",
      "\n",
      "TreebankWordTokenizer...\n",
      "['Petrol', 'price', 'has', 'gone', 'upto', 'Rs.75.89', 'I', ',', 'John', 'and', 'Mrs.', 'Thomas', 'are', 'thinking', 'of', 'using', 'electric', 'scooters', '.']\n",
      "\n",
      "\n",
      "\n",
      "Sentence: @Nikes: This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\n",
      "['@', 'Nikes', ':', 'This', 'is', 'a', 'cooool', '#', 'dummysmiley', ':', ':', '-', ')', ':', '-P', '<', '3', 'and', 'some', 'arrows', '<', '>', '-', '>', '<', '--']\n"
     ]
    }
   ],
   "source": [
    "s = \"Petrol price has gone upto Rs.75.89 I,John and Mrs. Thomas are thinking of using electric scooters.\"\n",
    "print(\"Sentence: \"+s)\n",
    "print(\"\\nTreebankWordTokenizer...\")\n",
    "print(TreebankWordTokenizer().tokenize(s))\n",
    "print(\"\\n\")\n",
    "\n",
    "#s = \"@someone did you check out this #superawesome!! it's very cool \\xF0\\x9F\\x98\\x81 http://t.co/ydfY2\"\n",
    "#print(\"Sentence: \"+s)\n",
    "#print(TreebankWordTokenizer().tokenize(s))\n",
    "#print(\"\\n\")\n",
    "\n",
    "#s = \"@Nike's quest to break the 2-hour marathon barrier is LIVE on Twitter. #Breaking2\"\n",
    "s= \"@Nikes: This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "print(\"\\nSentence: \"+s)\n",
    "print(TreebankWordTokenizer().tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The previous tokenizers fail badly for tweets, TweetTokenizer can be used to tokenize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Nike',\n",
       " ':',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "s0 = \"@Nike: This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@someone', 'did', 'you', 'check', 'out', 'this', '#superawesome', '!', '!', \"it's\", 'very', 'cool', 'ð', '\\x9f', '\\x98', '\\x81', 'http://t.co/ydfY2']\n"
     ]
    }
   ],
   "source": [
    "tweet = str(\"@someone did you check out this #superawesome!! it's very cool \\xF0\\x9F\\x98\\x81 http://t.co/ydfY2\")\n",
    "print(tknzr.tokenize(tweet))\n",
    "\n",
    "s = str(\"@Nike's quest to break the 2-hour marathon barrier is LIVE on Twitter. #Breaking2\")\n",
    "#tknzr.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Python version are you using? If it is Python 2.x then tweet becomes a bytestring (while you should pass unicode to NLTK methods). If it is Python 3 you're getting str representation of a bytes object, this is not what you want:\n",
    "https://github.com/nltk/nltk/issues/1155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For Python 3.5\n",
    "tweet = b\"@someone did you check out this #superawesome!! it's very cool \\xF0\\x9F\\x98\\x81 http://t.co/ydfY2\".decode('utf-8')\n",
    "#tknzr.tokenize(bytes(tweet, 'utf-8').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usually we want to have phrases like New_York as a single word. This might be beneficial to certain downstream applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the gutenburg corpus\n",
    "corpus = []\n",
    "for fileid in gutenberg.fileids():\n",
    "    corpus.extend(gutenberg.sents(fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98552\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load Bigram Association measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the BigramAssocMeasures\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Collect the bigram statistics from the corpus\n",
    "finder = BigramCollocationFinder.from_documents(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(~),', 'asterisk'), ('1000', '1997'), ('10000', '2004'), ('1500', '1998'), ('1739', 'University'), ('217', 'Peabody'), ('26th', 'ult'), ('9000', '2003'), ('AH', 'SUNFLOWER'), ('AN', 'INTERLUDE'), ('ANCIENT', 'BARD'), ('ANNUS', 'MIRABILIS'), ('AUGUST', '3d'), ('AUTUMN', 'RIVULETS'), ('Abhorred', 'Styx'), ('Adders', 'Forke'), ('Adult', 'Reformatory'), ('Agnus', 'Dei'), ('Alexandrian', 'Pharos'), ('Ally', 'Sloper')]\n"
     ]
    }
   ],
   "source": [
    "#Use say Chi-Squared test to extract top-k bigram candidates\n",
    "collocs = finder.nbest(bigram_measures.dice, 200)\n",
    "print(collocs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import MWETokenizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize with the previously collected collocation\n",
    "tokenizer = MWETokenizer(collocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = 'The Tower of Hercules, near A Coruña in Spain, a 2nd century AD Roman lighthouse, is closely modelled on the Alexandrian Pharos'\n",
    "tokenizer.tokenize(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We wan't 2nd century AD to be a single token\n",
    "# print(collocs)\n",
    "collocs.append(('2nd', 'century', 'AD'))\n",
    "tokenizer = MWETokenizer(collocs)\n",
    "sentence = 'The Tower of Hercules, near A Coruña in Spain, a 2nd century AD Roman lighthouse, is closely modelled on the Alexandrian Pharos'\n",
    "tokenizer.tokenize(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nationalism\n"
     ]
    }
   ],
   "source": [
    "#**WordNet Lemmatizer**\n",
    "#Lemmatize using WordNet’s built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('nationalism'))\n",
    "#print(wnl.lemmatize('loving','v'))\n",
    "#print(wnl.lemmatize('went', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "#**SnowballStemmer**\n",
    "#For Snowball Stemmer, which is based on Snowball Stemming Algorithm, can be used in NLTK like this:\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nation'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')\n",
    "#snowball_stemmer.stem('maximum')\n",
    "#snowball_stemmer.stem('presumably')\n",
    "snowball_stemmer.stem('nationalism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import GermanStemmer\n",
    "stemmer = GermanStemmer()\n",
    "stemmer.stem(\"Autobahnen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#more details and examples here\n",
    "# http://www.nltk.org/api/nltk.tokenize.html1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

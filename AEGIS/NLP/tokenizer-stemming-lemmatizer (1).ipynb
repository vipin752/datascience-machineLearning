{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this tutorial, we will look at some of the tokenizers available in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "\n",
      "Good muffins cost $3.80 in New York.\n",
      "Dr. Ram Please buy me two of them.\n",
      "Thanks.\n",
      "\n",
      "word_tokenize output\n",
      "['Good', 'muffins', 'cost', '$', '3.80', 'in', 'New', 'York', '.', 'Dr.', 'Ram', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "\n",
      "split tokenize output\n",
      "['Good', 'muffins', 'cost', '$3.80', 'in', 'New', 'York.', 'Dr.', 'Ram', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenization using NLTK\n",
    "# word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = \"Good muffins cost $3.80 in New York.\\nDr. Ram Please buy me two of them.\\nThanks.\"\n",
    "print(\"Sentence: \\n\\n\"+s) \n",
    "print(\"\\nword_tokenize output\")\n",
    "print(word_tokenize(s))\n",
    "\n",
    "print(\"\\nsplit tokenize output\")\n",
    "print(s.split())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "\n",
      "Good muffins cost $3.80 in New York.\n",
      "Dr. Ram Please buy me two of them.\n",
      "Thanks.\n",
      "\n",
      "wordpunct_tokenize output\n",
      "['Good', 'muffins', 'cost', '$', '3', '.', '80', 'in', 'New', 'York', '.', 'Dr', '.', 'Ram', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "s = \"Good muffins cost $3.80 in New York.\\nDr. Ram Please buy me two of them.\\nThanks.\"\n",
    "print(\"Sentence: \\n\\n\"+s) \n",
    "print(\"\\nwordpunct_tokenize output\")\n",
    "print(wordpunct_tokenize(s))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "\n",
      "Good muffins cost $3.80 in New York. Dr. Ram Please buy me two of them.Thanks.\n",
      "\n",
      "sent_tokenize output\n",
      "['Good muffins cost $3.80 in New York.', 'Dr. Ram Please buy me two of them.Thanks.']\n",
      "\n",
      "word_tokenize output\n",
      "['Good', 'muffins', 'cost', '$', '3.80', 'in', 'New', 'York', '.']\n",
      "['Dr.', 'Ram', 'Please', 'buy', 'me', 'two', 'of', 'them.Thanks', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "s2= \"Good muffins cost $3.80 in New York. Dr. Ram Please buy me two of them.Thanks.\"\n",
    "print(\"Sentence: \\n\\n\"+s2) \n",
    "print(\"\\nsent_tokenize output\")\n",
    "print(sent_tokenize(s2))\n",
    "print(\"\\nword_tokenize output\")\n",
    "for t in sent_tokenize(s2):\n",
    "    print(word_tokenize(t))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LineTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import LineTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LineTokenizer can be used to split strings containing newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: \n",
      "I love kites.\n",
      "I like cricket.\n",
      "I like football.\n",
      "\n",
      "LineTokenizer...\n",
      "['I love kites.', 'I like cricket.', 'I like football.']\n",
      "\n",
      "word_tokenizer... \n",
      "['I', 'love', 'kites', '.']\n",
      "['I', 'like', 'cricket', '.']\n",
      "['I', 'like', 'football', '.']\n"
     ]
    }
   ],
   "source": [
    "s = \"I love kites.\\nI like cricket.\\nI like football.\\n\"\n",
    "\n",
    "print(\"Sentences: \") \n",
    "print(s)\n",
    "print(\"LineTokenizer...\")\n",
    "print(LineTokenizer().tokenize(s))\n",
    "print(\"\\nword_tokenizer... \")\n",
    "for sent in LineTokenizer().tokenize(s):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegexpTokenizer allows us to provide regular expressions as delimiters\n",
    "The material between the tokens is discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Petrol price has gone upto Rs.75.89 on 01/02/2017. John and Mrs. Thomas are thinking of using electric scooters.\n",
      "\n",
      "RegexpTokenizer...\n",
      "['Rs.75.89']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = \"Petrol price has gone upto Rs.75.89 on 01/02/2017. John and Mrs. Thomas are thinking of using electric scooters.\"\n",
    "tokenizer = RegexpTokenizer('Rs\\.[\\d]+\\.[\\d]+')\n",
    "print(\"Sentence: \"+s)\n",
    "print(\"\\nRegexpTokenizer...\")\n",
    "print(tokenizer.tokenize(s))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Petrol', 'Rs.75.89', 'John', 'Mrs.', 'Thomas']\n"
     ]
    }
   ],
   "source": [
    "#Let us say we want to extract all words beginning with an uppercase character\n",
    "tokenizer = RegexpTokenizer('[A-Z]\\w*\\S+')\n",
    "print(tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SExprTokenizer : Tokenizes parenthesized expressions in a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import SExprTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ?(a(b c)d)ef(g(h(i)))\n",
      "\n",
      "SExprTokenizer...\n",
      "['?', '(a(b c)d)', 'ef', '(g(h(i)))']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = '?(a(b c)d)ef(g(h(i)))'\n",
    "print(\"Sentence: \"+s)\n",
    "print(\"\\nSExprTokenizer...\")\n",
    "print(SExprTokenizer().tokenize(s))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreebankWordTokenizer is standard tokenizer tool used and does a decent job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Good muffins cost $3.80 in New York. Dr. Ram Please buy me two of them. Thanks.\n",
      "\n",
      "TreebankWordTokenizer...\n",
      "['Good', 'muffins', 'cost', '$', '3.80', 'in', 'New', 'York.', 'Dr.', 'Ram', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = \"Good muffins cost $3.80 in New York. Dr. Ram Please buy me two of them. Thanks.\"\n",
    "print(\"Sentence: \"+s)\n",
    "print(\"\\nTreebankWordTokenizer...\")\n",
    "print(TreebankWordTokenizer().tokenize(s))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: @Nikes: This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\n",
      "['@', 'Nikes', ':', 'This', 'is', 'a', 'cooool', '#', 'dummysmiley', ':', ':', '-', ')', ':', '-P', '<', '3', 'and', 'some', 'arrows', '<', '>', '-', '>', '<', '--']\n"
     ]
    }
   ],
   "source": [
    "s= \"@Nikes: This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "print(\"\\nSentence: \"+s)\n",
    "print(TreebankWordTokenizer().tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The previous tokenizers fail badly for tweets, TweetTokenizer can be used to tokenize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Nike',\n",
       " ':',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "s0 = \"@Nike: This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed\n",
      "compute\n",
      "nationality\n"
     ]
    }
   ],
   "source": [
    "#**WordNet Lemmatizer**\n",
    "#Lemmatize using WordNet’s built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('computed'))\n",
    "print(wnl.lemmatize('computed','v'))\n",
    "print(wnl.lemmatize('nationality'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "#**SnowballStemmer**\n",
    "#For Snowball Stemmer, which is based on Snowball Stemming Algorithm, can be used in NLTK like this:\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comput\n",
      "nation\n"
     ]
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')\n",
    "#snowball_stemmer.stem('maximum')\n",
    "#snowball_stemmer.stem('presumably')\n",
    "print(snowball_stemmer.stem('computing'))\n",
    "print(snowball_stemmer.stem('nationality'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import GermanStemmer\n",
    "stemmer = GermanStemmer()\n",
    "stemmer.stem(\"Autobahnen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for more details and examples see http://www.nltk.org/api/nltk.tokenize.html1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

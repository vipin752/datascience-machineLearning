{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this tutorial we will train a sentiment classifier on a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################## Loading data file #######################\n",
    "reader_train = csv.reader(open('data/sentiment_analysis/training.csv','r'))\n",
    "reader_test = csv.reader(open('data/sentiment_analysis/test.csv','r'))\n",
    "training_data = []\n",
    "test_data = []\n",
    "header = 1\n",
    "for row in reader_train:\n",
    "        if header==1:\n",
    "                header=0\n",
    "                continue\n",
    "        training_data.append(row)\n",
    "header=1\n",
    "for row in reader_test:\n",
    "        if header==1:\n",
    "                header=0\n",
    "                continue\n",
    "        test_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Sorry, I\\\\'ve given GA several chances and it\\\\'s never delivered more than what it is: A predictable, prime time Melodrama geared to the feminine Soap Opera audience.\", 'neg']\n",
      "385 80\n"
     ]
    }
   ],
   "source": [
    "# Examples from training data\n",
    "print(training_data[1])\n",
    "print(len(training_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10495\n",
      "2759\n"
     ]
    }
   ],
   "source": [
    "#Required for Bag of words (unigram features) creation\n",
    "vocabulary = [x.lower() for tagged_sent in training_data for x in tagged_sent[0].split()]\n",
    "print(len(vocabulary))\n",
    "vocabulary = list(set(vocabulary))\n",
    "vocabulary.sort() #sorting the list\n",
    "print(len(vocabulary))\n",
    "# print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################## Extracting Features #########################\n",
    "#### Prepare a unigram feature vector based on the presence or absence of words######### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unigram_features(data,vocab):\n",
    "    fet_vec_all = []\n",
    "    for tup in data:\n",
    "        single_feat_vec = []\n",
    "        sent = tup[0].lower() #lowercasing the dataset\n",
    "        for v in vocab:\n",
    "            if sent.__contains__(v):\n",
    "                single_feat_vec.append(1)\n",
    "            else:\n",
    "                single_feat_vec.append(0)\n",
    "        fet_vec_all.append(single_feat_vec)\n",
    "    return fet_vec_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add sentiment scores from sentiwordnet, here we take the average sentiment scores of all words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_senti_wordnet_features(data):\n",
    "    fet_vec_all = []\n",
    "    for tup in data:\n",
    "        sent = tup[0].lower()\n",
    "        words = sent.split()\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        for w in words:\n",
    "            senti_synsets = swn.senti_synsets(w.lower())\n",
    "            for senti_synset in senti_synsets:\n",
    "                p = senti_synset.pos_score()\n",
    "                n = senti_synset.neg_score()\n",
    "                pos_score+=p\n",
    "                neg_score+=n\n",
    "                break #take only the first synset (Most frequent sense)\n",
    "        fet_vec_all.append([float(pos_score),float(neg_score)])\n",
    "    return fet_vec_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the two scores ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_features(featureList1,featureList2):\n",
    "    # For merging two features\n",
    "    if featureList1==[]:\n",
    "        return featureList2\n",
    "    merged = []\n",
    "    for i in range(len(featureList1)):\n",
    "        m = featureList1[i]+featureList2[i]\n",
    "        merged.append(m)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract the sentiment labels by making positive reviews as class 1 and negative reviews as class 2\n",
    "def get_lables(data):\n",
    "    labels = []\n",
    "    for tup in data:\n",
    "        if tup[1].lower()==\"neg\":\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_precision(prediction, actual):\n",
    "    prediction = list(prediction)\n",
    "    correct_labels = [predictions[i]  for i in range(len(predictions)) if actual[i] == predictions[i]]\n",
    "    precision = float(len(correct_labels))/float(len(prediction))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def real_time_test(classifier,vocab):\n",
    "    print(\"Enter a sentence: \")\n",
    "    inp = input()\n",
    "    print(inp)\n",
    "    feat_vec_uni = get_unigram_features(inp,vocab)\n",
    "    feat_vec_swn =get_senti_wordnet_features(test_data)\n",
    "    feat_vec = merge_features(feat_vec_uni, feat_vec_swn)\n",
    "\n",
    "    predict = classifier.predict(feat_vec)\n",
    "    if predict[0]==1:\n",
    "        print(\"The sentiment expressed is: positive\")\n",
    "    else:\n",
    "        print(\"The sentiment expressed is: negative\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################# Training and Evaluation #######################\n",
    "#### Preparing training and test tuples\n",
    "#### The feature_vecor set looks like [featurevector1, featurevector2,...,featurevectorN] where each featurevectorX is a list\n",
    "#### The label set looks like [label1,label2,...,labelN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_unigram_features = get_unigram_features(training_data,vocabulary) # vocabulary extracted in the beginning\n",
    "training_swn_features = get_senti_wordnet_features(training_data)\n",
    "\n",
    "training_features = merge_features(training_unigram_features,training_swn_features)\n",
    "\n",
    "training_labels = get_lables(training_data)\n",
    "\n",
    "test_unigram_features = get_unigram_features(test_data,vocabulary)\n",
    "test_swn_features=get_senti_wordnet_features(test_data)\n",
    "test_features= merge_features(test_unigram_features,test_swn_features)\n",
    "\n",
    "test_gold_labels = get_lables(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of NB classifier is\n",
      "Training data\t0.987012987012987\n",
      "Test data\t0.775\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_classifier = MultinomialNB().fit(training_features,training_labels) #training process\n",
    "predictions = nb_classifier.predict(test_features)\n",
    "\n",
    "print(\"Precision of NB classifier is\")\n",
    "predictions = nb_classifier.predict(training_features)\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = nb_classifier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: \n",
      "I like movie\n",
      "I like movie\n",
      "The sentiment expressed is: negative\n"
     ]
    }
   ],
   "source": [
    "#Real time tesing\n",
    "real_time_test(nb_classifier,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of linear SVM classifier is:\n",
      "Training data\t0.9584415584415584\n",
      "Test data\t0.775\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "#Refer to : http://scikit-learn.org/stable/modules/svm.html\n",
    "from sklearn.svm import LinearSVC\n",
    "svm_classifier = LinearSVC(penalty='l2', C=0.01).fit(training_features,training_labels)\n",
    "predictions = svm_classifier.predict(training_features)\n",
    "\n",
    "print(\"Precision of linear SVM classifier is:\")\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = svm_classifier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: \n",
      "movie is awesome\n",
      "movie is awesome\n",
      "The sentiment expressed is: positive\n"
     ]
    }
   ],
   "source": [
    "#Real time tesing\n",
    "real_time_test(svm_classifier,vocabulary)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
